package main.com.test

import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.{SparkConf, SparkContext}

object StreamingTest {
  def simpleFileStreaming(): Unit = {
    val conf = new SparkConf()
      .setAppName("StreamingTest")
    val sc = new SparkContext(conf)
    val ssc = new StreamingContext(sc, Seconds(20))
    val lines = ssc.textFileStream("file:///home/ubuntu/myTest")
    val words = lines.flatMap(_.split(" "))
    val wordCounts = words.map(x => (x, 1)).reduceByKey(_ + _)
    wordCounts.print()
    ssc.start()
    //实际上，当你输入这行回车后，Spark Streaming就开始进行循环监听，
    // 下面的ssc.awaitTermination()是无法输入到屏幕上的，
    // 但是，为了程序完整性，这里还是给出ssc.awaitTermination()
    ssc.awaitTermination()
  }

  def simpleAppStreaming(): Unit = {
    println("作为监听器(1)还是发信器(2)？")
    val tmp = scala.io.StdIn.readLine()
    if (tmp == "1") {
      receiver()
    } else if (tmp == "2") {
      sender()
    }
    println("进程结束")
  }

  def sender(): Unit = {

  }

  def receiver(): Unit = {
    println("请设置监听地址与端口号，用空格隔开：")
    val ipAndPort = scala.io.StdIn.readLine().split(" ")
    val sparkConf = new SparkConf().setAppName("NetworkWordCount").setMaster("local[2]")
    val ssc = new StreamingContext(sparkConf, Seconds(1))
    // Create a socket stream on target ip:port and count the
    // words in input stream of \n delimited text (eg. generated by 'nc')
    // Note that no duplication in storage level only for running locally.
    // Replication necessary in distributed scenario for fault tolerance.
    val lines = ssc.socketTextStream(ipAndPort(0), ipAndPort(1).toInt, StorageLevel.MEMORY_AND_DISK_SER)
    val words = lines.flatMap(_.split(" "))
    val wordCounts = words.map(x => (x, 1)).reduceByKey(_ + _)
    wordCounts.print()
    ssc.start()
    ssc.awaitTermination()
  }
}
